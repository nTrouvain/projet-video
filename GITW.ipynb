{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# A DEFINIR AVANT L'ENTRAINEMENT ET L'INFERENCE\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GITW Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Background',\n",
       " 'Bowl',\n",
       " 'CanOfCocaCola',\n",
       " 'FryingPan',\n",
       " 'Glass',\n",
       " 'Jam',\n",
       " 'Lid',\n",
       " 'MilkBottle',\n",
       " 'Mug',\n",
       " 'OilBottle',\n",
       " 'Plate',\n",
       " 'Rice',\n",
       " 'SaucePan',\n",
       " 'Sponge',\n",
       " 'Sugar',\n",
       " 'VinegarBottle',\n",
       " 'WashLiquid']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gitw_path = path.join(\"data\", \"GITW_clean\")\n",
    "classes = [d for d in os.listdir(gitw_path) if path.isdir(path.join(gitw_path, d))]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = []\n",
    "files = {\n",
    "    \"file\": [],\n",
    "    \"label\": [],\n",
    "    \"frame\": []\n",
    "}\n",
    "\n",
    "for c_name in classes:\n",
    "    for img in os.listdir(path.join(gitw_path, c_name)):\n",
    "        if path.isfile(path.join(gitw_path, c_name, img)):\n",
    "            files[\"file\"].append(path.join(c_name, img))\n",
    "            files[\"label\"].append(classes.index(c_name))\n",
    "            files[\"frame\"].append(int(img.split(\"_\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Background\\Frame_0320_bg.png</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Background\\Frame_0360_bg.png</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Background\\Frame_0400_bg.png</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Background\\Frame_0440_bg.png</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background\\Frame_0480_bg.png</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file  label  frame\n",
       "0  Background\\Frame_0320_bg.png      0    320\n",
       "1  Background\\Frame_0360_bg.png      0    360\n",
       "2  Background\\Frame_0400_bg.png      0    400\n",
       "3  Background\\Frame_0440_bg.png      0    440\n",
       "4  Background\\Frame_0480_bg.png      0    480"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = pd.DataFrame(files).sort_values(by=[\"label\", \"frame\"]).reset_index(drop=True)\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4451, 786)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = train_test_split(files.index, test_size=0.15)\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les liens vers les donnnées de test et d'entraînement sont sauvegardés dans `train_dataset.csv` et `test_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = files.loc[x_train], files.loc[x_test]\n",
    "train_df.to_csv(path.join(gitw_path, \"train_dataset.csv\"), index=False)\n",
    "test_df.to_csv(path.join(gitw_path, \"test_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes GITWDataset et GITWInferenceSet permettent de charger les données depuis le dossier racine dynamiquement, pour l'entraînement comme pour l'inférence. Les deux objets génèrent des *batchs* de taille `batch_size` contenant des tuples (image, classe) pour l'entraînement et des images pour l'inférence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GITWDataset(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generateur de données d'entraînement\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, df, classes, batch_size=32):\n",
    "        self.X = [path.join(data_path, file) for file in df[\"file\"].values]\n",
    "        self.Y = keras.utils.to_categorical(df[\"label\"].values)\n",
    "        self.batch_size = batch_size\n",
    "        self.path = data_path\n",
    "        self.df = df\n",
    "        self.classes = classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.df) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        files_x = self.X[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_y = self.Y[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_x = []\n",
    "        for file in files_x:\n",
    "            frame = np.array(Image.open(os.path.join(file)).convert(\"RGB\").resize((224,224), Image.BICUBIC).getdata())\n",
    "            batch_x.append(frame.reshape(224, 224, 3) / 255) # normalisation\n",
    "        return np.stack(batch_x), batch_y\n",
    "    \n",
    "class GITWInferenceSet(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Générateur de données pour l'inférence\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, df, classes, batch_size=32):\n",
    "        self.X = [path.join(data_path, file) for file in df[\"file\"].values]\n",
    "        self.Y = keras.utils.to_categorical(df[\"label\"].values)\n",
    "        self.batch_size = batch_size\n",
    "        self.path = data_path\n",
    "        self.df = df\n",
    "        self.classes = classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.df) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        files_x = self.X[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_x = []\n",
    "        for file in files_x:\n",
    "            frame = np.array(Image.open(os.path.join(file)).convert(\"RGB\").resize((224,224), Image.BICUBIC).getdata())\n",
    "            batch_x.append(frame.reshape(224, 224, 3) / 255)\n",
    "        return np.stack(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = GITWDataset(gitw_path, train_df, classes, batch_size=batch_size)\n",
    "testset = GITWDataset(gitw_path, test_df, classes, batch_size=batch_size)\n",
    "inferenceset = GITWInferenceSet(gitw_path, test_df, classes, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_module = ResNet50(include_top=False, input_shape=(224, 224, 3), )\n",
    "resnet = models.Sequential([\n",
    "    resnet_module,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(len(classes), activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17)                1706001   \n",
      "=================================================================\n",
      "Total params: 25,293,713\n",
      "Trainable params: 25,240,593\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "279/279 [==============================] - 297s 1s/step - loss: 3.6505 - accuracy: 0.3876 - val_loss: 13.3320 - val_accuracy: 0.0980\n",
      "Epoch 2/20\n",
      "279/279 [==============================] - 292s 1s/step - loss: 2.2665 - accuracy: 0.6129 - val_loss: 2.4957 - val_accuracy: 0.5407\n",
      "Epoch 3/20\n",
      "279/279 [==============================] - 300s 1s/step - loss: 2.0806 - accuracy: 0.6457 - val_loss: 14.3799 - val_accuracy: 0.0980\n",
      "Epoch 4/20\n",
      "279/279 [==============================] - 289s 1s/step - loss: 2.1587 - accuracy: 0.6284 - val_loss: 1.5320 - val_accuracy: 0.6056\n",
      "Epoch 5/20\n",
      "279/279 [==============================] - 290s 1s/step - loss: 2.3707 - accuracy: 0.5904 - val_loss: 1.1454 - val_accuracy: 0.6858\n",
      "Epoch 6/20\n",
      "279/279 [==============================] - 293s 1s/step - loss: 1.8323 - accuracy: 0.7261 - val_loss: 3.4437 - val_accuracy: 0.6272\n",
      "Epoch 7/20\n",
      "279/279 [==============================] - 288s 1s/step - loss: 1.5407 - accuracy: 0.7965 - val_loss: 1.8656 - val_accuracy: 0.7328\n",
      "Epoch 8/20\n",
      "279/279 [==============================] - 288s 1s/step - loss: 1.4728 - accuracy: 0.8275 - val_loss: 1.5900 - val_accuracy: 0.7494\n",
      "Epoch 9/20\n",
      "279/279 [==============================] - 283s 1s/step - loss: 1.4559 - accuracy: 0.8234 - val_loss: 1.4482 - val_accuracy: 0.7659\n",
      "Epoch 10/20\n",
      "279/279 [==============================] - 275s 985ms/step - loss: 1.2747 - accuracy: 0.8731 - val_loss: 0.9969 - val_accuracy: 0.7710\n",
      "Epoch 11/20\n",
      "279/279 [==============================] - 285s 1s/step - loss: 1.3348 - accuracy: 0.8582 - val_loss: 5.1548 - val_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "279/279 [==============================] - 290s 1s/step - loss: 1.2625 - accuracy: 0.8367 - val_loss: 1.1044 - val_accuracy: 0.7672\n",
      "Epoch 13/20\n",
      "279/279 [==============================] - 290s 1s/step - loss: 1.3633 - accuracy: 0.8358 - val_loss: 3.8898 - val_accuracy: 0.5585\n",
      "Epoch 14/20\n",
      "279/279 [==============================] - 284s 1s/step - loss: 1.8852 - accuracy: 0.7223 - val_loss: 12.4724 - val_accuracy: 0.1323\n",
      "Epoch 15/20\n",
      "279/279 [==============================] - 279s 1s/step - loss: 1.7596 - accuracy: 0.7565 - val_loss: 13.5255 - val_accuracy: 0.1056\n",
      "Epoch 16/20\n",
      "279/279 [==============================] - 281s 1s/step - loss: 1.5007 - accuracy: 0.8009 - val_loss: 12.4876 - val_accuracy: 0.1260\n",
      "Epoch 17/20\n",
      "279/279 [==============================] - 281s 1s/step - loss: 1.5551 - accuracy: 0.7704 - val_loss: 1.6320 - val_accuracy: 0.7455\n",
      "Epoch 18/20\n",
      "279/279 [==============================] - 279s 1s/step - loss: 1.4114 - accuracy: 0.8288 - val_loss: 2.5329 - val_accuracy: 0.6883\n",
      "Epoch 19/20\n",
      "279/279 [==============================] - 282s 1s/step - loss: 1.1854 - accuracy: 0.8607 - val_loss: 1.8226 - val_accuracy: 0.7303\n",
      "Epoch 20/20\n",
      "279/279 [==============================] - 274s 982ms/step - loss: 1.2906 - accuracy: 0.8625 - val_loss: 0.8820 - val_accuracy: 0.8104\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=path.join(\".\", \"logs\", \"resnet\"), update_freq=50) # Logs d'entraînement\n",
    "reducer = ReduceLROnPlateau(monitor='val_accuracy') # Si l'accuracy de validation stagne, réduit le lr d'un facteur 0.1\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-3, decay=1e-8) # choisi pour son efficacité et parce qu'il a déjà servi à entraîner ResNet et VGG\n",
    "\n",
    "resnet.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "resnet_history = resnet.fit_generator(trainset, \n",
    "                    epochs=20,\n",
    "                    validation_data=testset, # La validation est effectuée avec les données de test directement\n",
    "                    callbacks=[tensorboard, reducer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19 - Blocks 4 et 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_module = VGG19(include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Les paramètres des blocs 1 à 3 sont fixés\n",
    "for layer in vgg_module.layers:\n",
    "    if (\"block1\" in layer.name) or (\"block2\" in layer.name) or (\"block3\" in layer.name):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17)                426513    \n",
      "=================================================================\n",
      "Total params: 20,450,897\n",
      "Trainable params: 18,125,329\n",
      "Non-trainable params: 2,325,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = models.Sequential([\n",
    "    vgg_module,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(len(classes), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "557/557 [==============================] - 537s 964ms/step - loss: 2.6952 - accuracy: 0.1968 - val_loss: 2.1852 - val_accuracy: 0.2888\n",
      "Epoch 2/20\n",
      "557/557 [==============================] - 547s 982ms/step - loss: 1.3882 - accuracy: 0.5918 - val_loss: 0.9313 - val_accuracy: 0.7226\n",
      "Epoch 3/20\n",
      "557/557 [==============================] - 534s 959ms/step - loss: 0.7203 - accuracy: 0.7998 - val_loss: 0.6092 - val_accuracy: 0.8168\n",
      "Epoch 4/20\n",
      "557/557 [==============================] - 539s 967ms/step - loss: 0.4757 - accuracy: 0.8598 - val_loss: 0.5070 - val_accuracy: 0.8639\n",
      "Epoch 5/20\n",
      "557/557 [==============================] - 528s 948ms/step - loss: 0.3957 - accuracy: 0.8913 - val_loss: 0.5349 - val_accuracy: 0.8588\n",
      "Epoch 6/20\n",
      "557/557 [==============================] - 535s 961ms/step - loss: 0.2947 - accuracy: 0.9144 - val_loss: 0.4917 - val_accuracy: 0.8664\n",
      "Epoch 7/20\n",
      "557/557 [==============================] - 533s 957ms/step - loss: 0.2401 - accuracy: 0.9299 - val_loss: 0.5290 - val_accuracy: 0.8626\n",
      "Epoch 8/20\n",
      "557/557 [==============================] - 536s 962ms/step - loss: 0.1977 - accuracy: 0.9423 - val_loss: 0.5131 - val_accuracy: 0.8804\n",
      "Epoch 9/20\n",
      "557/557 [==============================] - 530s 952ms/step - loss: 0.2834 - accuracy: 0.9193 - val_loss: 0.3859 - val_accuracy: 0.9008\n",
      "Epoch 10/20\n",
      "557/557 [==============================] - 532s 956ms/step - loss: 0.1801 - accuracy: 0.9488 - val_loss: 0.6488 - val_accuracy: 0.8601\n",
      "Epoch 11/20\n",
      "557/557 [==============================] - 534s 959ms/step - loss: 0.2136 - accuracy: 0.9414 - val_loss: 0.8859 - val_accuracy: 0.8422\n",
      "Epoch 12/20\n",
      "557/557 [==============================] - 533s 957ms/step - loss: 0.1849 - accuracy: 0.9465 - val_loss: 0.4472 - val_accuracy: 0.9084\n",
      "Epoch 13/20\n",
      "557/557 [==============================] - 529s 950ms/step - loss: 0.1795 - accuracy: 0.9555 - val_loss: 0.6739 - val_accuracy: 0.8664\n",
      "Epoch 14/20\n",
      "557/557 [==============================] - 530s 951ms/step - loss: 0.1277 - accuracy: 0.9611 - val_loss: 0.4382 - val_accuracy: 0.9071\n",
      "Epoch 15/20\n",
      "557/557 [==============================] - 529s 950ms/step - loss: 0.0836 - accuracy: 0.9755 - val_loss: 0.4501 - val_accuracy: 0.8995\n",
      "Epoch 16/20\n",
      "557/557 [==============================] - 531s 954ms/step - loss: 0.1578 - accuracy: 0.9596 - val_loss: 0.6691 - val_accuracy: 0.8651\n",
      "Epoch 17/20\n",
      "557/557 [==============================] - 529s 950ms/step - loss: 0.1609 - accuracy: 0.9571 - val_loss: 0.7330 - val_accuracy: 0.8499\n",
      "Epoch 18/20\n",
      "557/557 [==============================] - 531s 954ms/step - loss: 0.1870 - accuracy: 0.9515 - val_loss: 0.3735 - val_accuracy: 0.9148\n",
      "Epoch 19/20\n",
      "557/557 [==============================] - 531s 953ms/step - loss: 0.1281 - accuracy: 0.9692 - val_loss: 0.5651 - val_accuracy: 0.8817\n",
      "Epoch 20/20\n",
      "557/557 [==============================] - 528s 948ms/step - loss: 0.1146 - accuracy: 0.9715 - val_loss: 0.7082 - val_accuracy: 0.8537\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=path.join(\".\", \"logs\", \"vgg\"), update_freq=50)\n",
    "reducer = ReduceLROnPlateau(monitor='val_accuracy')\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-4, decay=1e-8)\n",
    "\n",
    "vgg.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "vgg_history = vgg.fit_generator(trainset, \n",
    "                    epochs=20,\n",
    "                    validation_data=testset,\n",
    "                    callbacks=[tensorboard, reducer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19 - Block 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_module = VGG19(include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in vgg_module.layers:\n",
    "    if (\"block1\" in layer.name) or (\"block2\" in layer.name) or (\"block3\" in layer.name) or (\"block4\" in layer.name):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17)                426513    \n",
      "=================================================================\n",
      "Total params: 20,450,897\n",
      "Trainable params: 9,865,745\n",
      "Non-trainable params: 10,585,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = models.Sequential([\n",
    "    vgg_module,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(len(classes), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "557/557 [==============================] - 545s 978ms/step - loss: 1.0510 - accuracy: 0.7001 - val_loss: 0.6144 - val_accuracy: 0.8333\n",
      "Epoch 2/20\n",
      "557/557 [==============================] - 539s 967ms/step - loss: 0.3816 - accuracy: 0.8865 - val_loss: 0.5018 - val_accuracy: 0.8753\n",
      "Epoch 3/20\n",
      "557/557 [==============================] - 562s 1s/step - loss: 0.2767 - accuracy: 0.9216 - val_loss: 0.4459 - val_accuracy: 0.8766\n",
      "Epoch 4/20\n",
      "557/557 [==============================] - 546s 980ms/step - loss: 0.1987 - accuracy: 0.9389 - val_loss: 0.5223 - val_accuracy: 0.8969\n",
      "Epoch 5/20\n",
      "557/557 [==============================] - 542s 973ms/step - loss: 0.1676 - accuracy: 0.9463 - val_loss: 0.4018 - val_accuracy: 0.9160\n",
      "Epoch 6/20\n",
      "557/557 [==============================] - 534s 958ms/step - loss: 0.1387 - accuracy: 0.9578 - val_loss: 0.5502 - val_accuracy: 0.8855\n",
      "Epoch 7/20\n",
      "557/557 [==============================] - 537s 964ms/step - loss: 0.1308 - accuracy: 0.9641 - val_loss: 0.4725 - val_accuracy: 0.8919\n",
      "Epoch 8/20\n",
      "557/557 [==============================] - 534s 959ms/step - loss: 0.1768 - accuracy: 0.9492 - val_loss: 0.4095 - val_accuracy: 0.9122\n",
      "Epoch 9/20\n",
      "557/557 [==============================] - 533s 957ms/step - loss: 0.0839 - accuracy: 0.9737 - val_loss: 0.4417 - val_accuracy: 0.9084\n",
      "Epoch 10/20\n",
      "557/557 [==============================] - 535s 961ms/step - loss: 0.0529 - accuracy: 0.9829 - val_loss: 0.4442 - val_accuracy: 0.9135\n",
      "Epoch 11/20\n",
      "557/557 [==============================] - 537s 965ms/step - loss: 0.1040 - accuracy: 0.9676 - val_loss: 0.7354 - val_accuracy: 0.8830\n",
      "Epoch 12/20\n",
      "557/557 [==============================] - 541s 970ms/step - loss: 0.1775 - accuracy: 0.9524 - val_loss: 0.4208 - val_accuracy: 0.9122\n",
      "Epoch 13/20\n",
      "557/557 [==============================] - 540s 969ms/step - loss: 0.0805 - accuracy: 0.9793 - val_loss: 0.5092 - val_accuracy: 0.9237\n",
      "Epoch 14/20\n",
      "557/557 [==============================] - 537s 965ms/step - loss: 0.0736 - accuracy: 0.9778 - val_loss: 0.6430 - val_accuracy: 0.9020\n",
      "Epoch 15/20\n",
      "557/557 [==============================] - 539s 967ms/step - loss: 0.0938 - accuracy: 0.9778 - val_loss: 0.5995 - val_accuracy: 0.8982\n",
      "Epoch 16/20\n",
      "557/557 [==============================] - 540s 970ms/step - loss: 0.1582 - accuracy: 0.9645 - val_loss: 0.5012 - val_accuracy: 0.9148\n",
      "Epoch 17/20\n",
      "557/557 [==============================] - 556s 998ms/step - loss: 0.1128 - accuracy: 0.9730 - val_loss: 0.6960 - val_accuracy: 0.8969\n",
      "Epoch 18/20\n",
      "557/557 [==============================] - 538s 966ms/step - loss: 0.0672 - accuracy: 0.9814 - val_loss: 0.9555 - val_accuracy: 0.8690\n",
      "Epoch 19/20\n",
      "557/557 [==============================] - 537s 965ms/step - loss: 0.0730 - accuracy: 0.9814 - val_loss: 0.5915 - val_accuracy: 0.9071\n",
      "Epoch 20/20\n",
      "557/557 [==============================] - 545s 978ms/step - loss: 0.1155 - accuracy: 0.9739 - val_loss: 0.6319 - val_accuracy: 0.9109\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=path.join(\".\", \"logs\", \"vgg_bl5\"), update_freq=50)\n",
    "reducer = ReduceLROnPlateau(monitor='val_accuracy')\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-4, decay=1e-8)\n",
    "\n",
    "vgg.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "vgg_history = vgg.fit_generator(trainset, \n",
    "                    epochs=20,\n",
    "                    validation_data=testset,\n",
    "                    callbacks=[tensorboard, reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
